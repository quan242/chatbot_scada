{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from underthesea import text_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import py_vncorenlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-12 10:02:26 INFO  WordSegmenter:24 - Loading Word Segmentation model\n",
      "2024-01-12 10:02:26 INFO  PosTagger:23 - Loading POS Tagging model\n",
      "2024-01-12 10:02:30 INFO  NerRecognizer:34 - Loading NER model\n",
      "2024-01-12 10:02:47 INFO  DependencyParser:32 - Loading Dependency Parsing model\n"
     ]
    }
   ],
   "source": [
    "model = py_vncorenlp.VnCoreNLP(save_dir=\"/workspace/nlplab/quannd/scada-full-stack/chatbot-scada/vncorenlp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tôi hôm nay dậy làm từ rất sớm .\n",
      "['Tôi', 'hôm nay', 'dậy', 'làm', 'từ', 'rất', 'sớm', '.']\n",
      "Tôi :  0 -> 3\n",
      "hôm nay :  4 -> 11\n",
      "dậy :  12 -> 15\n",
      "làm :  16 -> 19\n",
      "từ :  20 -> 22\n",
      "rất :  23 -> 26\n",
      "sớm :  27 -> 30\n",
      ". :  31 -> 32\n"
     ]
    }
   ],
   "source": [
    "text = \"Tôi hôm nay dậy làm từ rất sớm.\"\n",
    "text = text_normalize(text)\n",
    "words = \" \".join(model.word_segment(text)).split(\" \")\n",
    "segmented_text = \" \".join(words)\n",
    "words = [word.replace(\"_\", \" \") for word in words]\n",
    "text = \" \".join(words)\n",
    "print(text)\n",
    "print(words)\n",
    "running_offset = 0\n",
    "for word in words:\n",
    "    try:\n",
    "        word_offset = text.index(word, running_offset)\n",
    "        word_len = len(word)\n",
    "        running_offset = word_offset + word_len\n",
    "        print(word,\": \", word_offset, \"->\", running_offset)\n",
    "    except:\n",
    "        print(f\"error at word: {word} and substring: {text[running_offset:]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tôi hôm_nay dậy làm từ rất sớm .'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segmented_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from underthesea import dependency_parse, chunk, pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('vẽ', 'V'),\n",
       " ('hình chữ nhật', 'N'),\n",
       " ('có', 'V'),\n",
       " ('chiều', 'N'),\n",
       " ('dài', 'A'),\n",
       " ('20.4 cm', 'N'),\n",
       " ('.', 'CH'),\n",
       " ('Hình', 'N'),\n",
       " ('này', 'P'),\n",
       " ('nằm', 'V'),\n",
       " ('ở', 'E'),\n",
       " ('góc', 'N'),\n",
       " ('trái', 'A'),\n",
       " ('trên', 'E'),\n",
       " ('màn hình', 'N'),\n",
       " ('.', 'CH')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag(\"vẽ hình chữ nhật có chiều dài 20.4 cm. Hình này nằm ở góc trái trên màn hình.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__all__',\n",
       " '__author__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__email__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__version__',\n",
       " 'chunk',\n",
       " 'classify',\n",
       " 'corpus',\n",
       " 'dependency_parse',\n",
       " 'feature_engineering',\n",
       " 'infile',\n",
       " 'models',\n",
       " 'ner',\n",
       " 'os',\n",
       " 'pipeline',\n",
       " 'pos_tag',\n",
       " 'sent_tokenize',\n",
       " 'sentiment',\n",
       " 'sys',\n",
       " 'text_normalize',\n",
       " 'util',\n",
       " 'version_file',\n",
       " 'version_info',\n",
       " 'word_tokenize']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(underthesea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/nlplab/quannd/rasa_env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "phobert_tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base-v2\")\n",
    "phobert_model = AutoModel.from_pretrained(\"vinai/phobert-base-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phobert_tokenizer.bos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import BertTokenizer\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"vinai/phobert-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import TFBertModel\n",
    "\n",
    "# model = TFBertModel.from_pretrained(\"vinai/phobert-base-v2\", from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "phobert_model = phobert_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad>'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phobert_tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"<s> \" + segmented_text + \" </s>\"\n",
    "\n",
    "encoded_info  = phobert_tokenizer.encode_plus(sentence,\n",
    "                                      padding='max_length',\n",
    "                                      add_special_tokens=False,\n",
    "                                      max_length=16,\n",
    "                                      return_tensors='np',\n",
    "                                      return_token_type_ids=False,\n",
    "                                      return_attention_mask=True)\n",
    "encoded_input = encoded_info['input_ids'].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': array([[   0,  218, 1113, 1764,   47,   39,   59,  471,    5,    2,    1,\n",
       "           1,    1,    1,    1,    1]]), 'attention_mask': array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " 'Tôi',\n",
       " 'hôm_nay',\n",
       " 'dậy',\n",
       " 'làm',\n",
       " 'từ',\n",
       " 'rất',\n",
       " 'sớm',\n",
       " '.',\n",
       " '</s>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phobert_tokenizer.convert_ids_to_tokens(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 746, 2]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'l à m'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phobert_tokenizer.decode(encoded_input[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< s >\n",
      "T ô i\n",
      "h ô m _ n a y\n",
      "d ậ y\n",
      "l à m\n",
      "t ừ\n",
      "r ấ t\n",
      "s ớ m\n",
      ".\n",
      "< / s >\n",
      "< p a d >\n",
      "< p a d >\n",
      "< p a d >\n",
      "< p a d >\n",
      "< p a d >\n",
      "< p a d >\n"
     ]
    }
   ],
   "source": [
    "for token in encoded_input:\n",
    "    print(phobert_tokenizer.decode(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([phobert_tokenizer.encode(sentence)])\n",
    "\n",
    "with torch.no_grad():\n",
    "    input_ids = input_ids.to(device)\n",
    "    features = phobert_model(input_ids)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 768])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 768)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[0].squeeze(0).cpu().numpy()[1:-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "abc_str = {'alias': None, 'config': 'vinai/phobert-base-v2'}\n",
    "print(type(abc_str) == dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Afsa\n"
     ]
    }
   ],
   "source": [
    "from typing import Any, Text, Dict\n",
    "def tmp_func(cls, config: Dict[Text, Any]) -> None:\n",
    "    print(config)\n",
    "    # for k, v in config.items():\n",
    "    #     print(k, v)\n",
    "\n",
    "tmp_func(4, \"Afsa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alias': 'abccccc', 'name': 'quan'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_dict = {'alias': 'abccccc'}\n",
    "{**tmp_dict, 'name': 'quan'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (4,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/workspace/nlplab/quannd/scada-full-stack/chatbot-scada/tmp.ipynb Cell 24\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bkcdichdangu.ddns.net/workspace/nlplab/quannd/scada-full-stack/chatbot-scada/tmp.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bkcdichdangu.ddns.net/workspace/nlplab/quannd/scada-full-stack/chatbot-scada/tmp.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m tmp_np \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49marray([[\u001b[39m1\u001b[39;49m, \u001b[39m2\u001b[39;49m, \u001b[39m4\u001b[39;49m, \u001b[39m5\u001b[39;49m],\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bkcdichdangu.ddns.net/workspace/nlplab/quannd/scada-full-stack/chatbot-scada/tmp.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m                    [\u001b[39m5\u001b[39;49m, \u001b[39m4\u001b[39;49m, \u001b[39m2\u001b[39;49m, \u001b[39m4\u001b[39;49m],\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bkcdichdangu.ddns.net/workspace/nlplab/quannd/scada-full-stack/chatbot-scada/tmp.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m                    [\u001b[39m4\u001b[39;49m, \u001b[39m2\u001b[39;49m, \u001b[39m3\u001b[39;49m],\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bkcdichdangu.ddns.net/workspace/nlplab/quannd/scada-full-stack/chatbot-scada/tmp.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m                    [\u001b[39m8\u001b[39;49m, \u001b[39m9\u001b[39;49m]])\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (4,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "tmp_np = np.array([[1, 2, 4, 5],\n",
    "                   [5, 4, 2, 4],\n",
    "                   [4, 2, 3],\n",
    "                   [8, 9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This class is not intended to be instantiated directly.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/workspace/nlplab/quannd/scada-full-stack/chatbot-scada/tmp.ipynb Cell 25\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bkcdichdangu.ddns.net/workspace/nlplab/quannd/scada-full-stack/chatbot-scada/tmp.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msparse\u001b[39;00m \u001b[39mimport\u001b[39;00m spmatrix\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bkcdichdangu.ddns.net/workspace/nlplab/quannd/scada-full-stack/chatbot-scada/tmp.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m sp_1 \u001b[39m=\u001b[39m spmatrix(\u001b[39m5\u001b[39;49m)\n",
      "File \u001b[0;32m/workspace/nlplab/quannd/rasa_env/lib/python3.8/site-packages/scipy/sparse/_base.py:108\u001b[0m, in \u001b[0;36mspmatrix.__init__\u001b[0;34m(self, maxprint)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shape \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mspmatrix\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 108\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mThis class is not intended\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    109\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39m to be instantiated directly.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    110\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmaxprint \u001b[39m=\u001b[39m maxprint\n",
      "\u001b[0;31mValueError\u001b[0m: This class is not intended to be instantiated directly."
     ]
    }
   ],
   "source": [
    "from scipy.sparse import spmatrix\n",
    "sp_1 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.8.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy\n",
    "scipy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bpemb\n",
      "  Downloading bpemb-0.3.4-py3-none-any.whl (19 kB)\n",
      "Collecting gensim (from bpemb)\n",
      "  Downloading gensim-4.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: numpy in /workspace/nlplab/quannd/rasa_env/lib/python3.8/site-packages (from bpemb) (1.24.4)\n",
      "Requirement already satisfied: requests in /workspace/nlplab/quannd/rasa_env/lib/python3.8/site-packages (from bpemb) (2.31.0)\n",
      "Collecting sentencepiece (from bpemb)\n",
      "  Downloading sentencepiece-0.1.99-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /workspace/nlplab/quannd/rasa_env/lib/python3.8/site-packages (from bpemb) (4.65.0)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /workspace/nlplab/quannd/rasa_env/lib/python3.8/site-packages (from gensim->bpemb) (1.8.1)\n",
      "Collecting smart-open>=1.8.1 (from gensim->bpemb)\n",
      "  Downloading smart_open-6.4.0-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /workspace/nlplab/quannd/rasa_env/lib/python3.8/site-packages (from requests->bpemb) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /workspace/nlplab/quannd/rasa_env/lib/python3.8/site-packages (from requests->bpemb) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /workspace/nlplab/quannd/rasa_env/lib/python3.8/site-packages (from requests->bpemb) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /workspace/nlplab/quannd/rasa_env/lib/python3.8/site-packages (from requests->bpemb) (2023.7.22)\n",
      "Downloading gensim-4.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0mm\n",
      "\u001b[?25hDownloading smart_open-6.4.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece, smart-open, gensim, bpemb\n",
      "Successfully installed bpemb-0.3.4 gensim-4.3.2 sentencepiece-0.1.99 smart-open-6.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install bpemb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bpemb import BPEmb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading https://nlp.h-its.org/bpemb/vi/vi.wiki.bpe.vs10000.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 387772/387772 [00:00<00:00, 435577.12B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading https://nlp.h-its.org/bpemb/vi/vi.wiki.bpe.vs10000.d100.w2v.bin.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3774870/3774870 [00:01<00:00, 2506606.89B/s]\n"
     ]
    }
   ],
   "source": [
    "model = BPEmb(\n",
    "    lang=\"vi\",\n",
    "    dim=100,\n",
    "    vs=10000,\n",
    "    vs_fallback=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Text\n",
    "import numpy as np\n",
    "\n",
    "def _create_word_vector(document: Text) -> np.ndarray:\n",
    "    \"\"\"Creates a word vector from a text. Utility method.\"\"\"\n",
    "    encoded_ids = model.encode_ids(document)\n",
    "    if encoded_ids:\n",
    "        return model.vectors[encoded_ids[0]]\n",
    "\n",
    "    return np.zeros((100,), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_sent = \"Trên bến dưới thuyền\"\n",
    "tokens = tmp_sent.split(\" \")\n",
    "word_vectors = np.array(\n",
    "    [_create_word_vector(document=token) for token in tokens]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 100)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hôm nay tôi đi chơi bóng bàn\n",
      "['hôm nay', 'tôi', 'đi', 'chơi', 'bóng bàn']\n",
      "hôm nay tôi đến rạp chiếu phim và ăn pizza\n",
      "['hôm nay', 'tôi', 'đến', 'rạp', 'chiếu phim', 'và', 'ăn', 'pizza']\n"
     ]
    }
   ],
   "source": [
    "sent_1 = \"hôm nay tôi đi chơi bóng bàn\"\n",
    "sent_2 = \"hôm nay tôi đến rạp chiếu phim và ăn pizza\"\n",
    "\n",
    "sent_1 = text_normalize(sent_1)\n",
    "words = \" \".join(model.word_segment(sent_1)).split(\" \")\n",
    "segmented_sent_1 = \" \".join(words)\n",
    "words = [word.replace(\"_\", \" \") for word in words]\n",
    "sent_1 = \" \".join(words)\n",
    "print(sent_1)\n",
    "print(words)\n",
    "\n",
    "sent_2 = text_normalize(sent_2)\n",
    "words = \" \".join(model.word_segment(sent_2)).split(\" \")\n",
    "segmented_sent_2 = \" \".join(words)\n",
    "words = [word.replace(\"_\", \" \") for word in words]\n",
    "sent_2 = \" \".join(words)\n",
    "print(sent_2)\n",
    "print(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hôm_nay tôi đi chơi bóng_bàn , hôm_nay tôi đến rạp chiếu_phim và ăn pizza\n"
     ]
    }
   ],
   "source": [
    "print(segmented_sent_1,\",\", segmented_sent_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_1 = phobert_tokenizer.encode(segmented_sent_1)\n",
    "ids_2 = phobert_tokenizer.encode(segmented_sent_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phobert_tokenizer.convert_ids_to_tokens(ids_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 15\n",
    "\n",
    "\n",
    "attn_mask_1 = [1] * len(ids_1) + [0] * (max_len - len(ids_1))\n",
    "ids_1 += [phobert_tokenizer.pad_token_id] *(max_len - len(ids_1))\n",
    "\n",
    "attn_mask_2 = [1] * len(ids_2) + [0] * (max_len - len(ids_2))\n",
    "ids_2 += [phobert_tokenizer.pad_token_id] *(max_len - len(ids_2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1113, 70, 57, 379, 11240, 2, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_mask_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [ids_1, ids_2]\n",
    "attn_masks = [attn_mask_1, attn_mask_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, SequentialSampler, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2408076/3712379048.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(inputs)\n",
      "/tmp/ipykernel_2408076/3712379048.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn_masks = torch.tensor(attn_masks)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "inputs = torch.tensor(inputs)\n",
    "attn_masks = torch.tensor(attn_masks)\n",
    "        \n",
    "data = TensorDataset(inputs, attn_masks)\n",
    "sampler = SequentialSampler(data)\n",
    "dataloader = DataLoader(data, sampler=sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.sampler.SequentialSampler at 0x7fbf0d261f70>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  7.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,  1113,    70,    57,   379, 11240,     2,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1],\n",
      "        [    0,  1113,    70,    30,  3752,  8458,     6,   203, 17080,     2,\n",
      "             1,     1,     1,     1,     1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for batch in tqdm(dataloader):\n",
    "        inputs, attn_masks = batch\n",
    "        print(inputs)\n",
    "        print(attn_masks)\n",
    "        inputs = inputs.to(device)\n",
    "        attn_masks = attn_masks.to(device)\n",
    "        features = phobert_model(inputs, attn_masks)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 768])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.last_hidden_state[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 768])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.pooler_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('abc', 1), ('agb', 2), ('gvc', 3), ('dxa', 4)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_arr_1 = [\"abc\", \"agb\", \"gvc\", \"dxa\"]\n",
    "tmp_arr_2 = [1, 2, 3, 4]\n",
    "\n",
    "list(zip(tmp_arr_1, tmp_arr_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text', 'response', 'action_text']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rasa.nlu.constants import DENSE_FEATURIZABLE_ATTRIBUTES\n",
    "DENSE_FEATURIZABLE_ATTRIBUTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text action_text\n"
     ]
    }
   ],
   "source": [
    "from rasa.shared.nlu.constants import TEXT, ACTION_TEXT\n",
    "print(TEXT, ACTION_TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tọa độ'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from underthesea import text_normalize\n",
    "text_normalize(\"tọa độ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import underthesea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_1 = \"hôm nay tôi đi chơi bóng bàn\"\n",
    "sent_2 = \"hôm nay tôi đến rạp chiếu phim và ăn pizza\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from underthesea import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hôm nay', 'tôi', 'đi', 'chơi', 'bóng', 'bàn']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sent_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/nlplab/quannd/rasa_env_3.4/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "model.bin:   2%|▏         | 115M/7.24G [00:21<22:05, 5.38MB/s]   \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/workspace/nlplab/quannd/scada-full-stack/chatbot-scada/tmp.ipynb Cell 59\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bkcdichdangu.ddns.net/workspace/nlplab/quannd/scada-full-stack/chatbot-scada/tmp.ipynb#Y112sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mfasttext\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bkcdichdangu.ddns.net/workspace/nlplab/quannd/scada-full-stack/chatbot-scada/tmp.ipynb#Y112sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mhuggingface_hub\u001b[39;00m \u001b[39mimport\u001b[39;00m hf_hub_download\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bkcdichdangu.ddns.net/workspace/nlplab/quannd/scada-full-stack/chatbot-scada/tmp.ipynb#Y112sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m model_path \u001b[39m=\u001b[39m hf_hub_download(repo_id\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mfacebook/fasttext-vi-vectors\u001b[39;49m\u001b[39m\"\u001b[39;49m, filename\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmodel.bin\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bkcdichdangu.ddns.net/workspace/nlplab/quannd/scada-full-stack/chatbot-scada/tmp.ipynb#Y112sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m model \u001b[39m=\u001b[39m fasttext\u001b[39m.\u001b[39mload_model(model_path)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bkcdichdangu.ddns.net/workspace/nlplab/quannd/scada-full-stack/chatbot-scada/tmp.ipynb#Y112sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m model\u001b[39m.\u001b[39mwords\n",
      "File \u001b[0;32m/workspace/nlplab/quannd/rasa_env_3.4/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/workspace/nlplab/quannd/rasa_env_3.4/lib/python3.8/site-packages/huggingface_hub/file_download.py:1461\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[1;32m   1458\u001b[0m         \u001b[39mif\u001b[39;00m local_dir \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1459\u001b[0m             _check_disk_space(expected_size, local_dir)\n\u001b[0;32m-> 1461\u001b[0m     http_get(\n\u001b[1;32m   1462\u001b[0m         url_to_download,\n\u001b[1;32m   1463\u001b[0m         temp_file,\n\u001b[1;32m   1464\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1465\u001b[0m         resume_size\u001b[39m=\u001b[39;49mresume_size,\n\u001b[1;32m   1466\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m   1467\u001b[0m         expected_size\u001b[39m=\u001b[39;49mexpected_size,\n\u001b[1;32m   1468\u001b[0m     )\n\u001b[1;32m   1470\u001b[0m \u001b[39mif\u001b[39;00m local_dir \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1471\u001b[0m     logger\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mStoring \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m in cache at \u001b[39m\u001b[39m{\u001b[39;00mblob_path\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/workspace/nlplab/quannd/rasa_env_3.4/lib/python3.8/site-packages/huggingface_hub/file_download.py:541\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, expected_size, _nb_retries)\u001b[0m\n\u001b[1;32m    539\u001b[0m new_resume_size \u001b[39m=\u001b[39m resume_size\n\u001b[1;32m    540\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 541\u001b[0m     \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m r\u001b[39m.\u001b[39miter_content(chunk_size\u001b[39m=\u001b[39mDOWNLOAD_CHUNK_SIZE):\n\u001b[1;32m    542\u001b[0m         \u001b[39mif\u001b[39;00m chunk:  \u001b[39m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[1;32m    543\u001b[0m             progress\u001b[39m.\u001b[39mupdate(\u001b[39mlen\u001b[39m(chunk))\n",
      "File \u001b[0;32m/workspace/nlplab/quannd/rasa_env_3.4/lib/python3.8/site-packages/requests/models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    815\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m     \u001b[39mexcept\u001b[39;00m ProtocolError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    818\u001b[0m         \u001b[39mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m/workspace/nlplab/quannd/rasa_env_3.4/lib/python3.8/site-packages/urllib3/response.py:934\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    933\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m is_fp_closed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp) \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 934\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(amt\u001b[39m=\u001b[39;49mamt, decode_content\u001b[39m=\u001b[39;49mdecode_content)\n\u001b[1;32m    936\u001b[0m         \u001b[39mif\u001b[39;00m data:\n\u001b[1;32m    937\u001b[0m             \u001b[39myield\u001b[39;00m data\n",
      "File \u001b[0;32m/workspace/nlplab/quannd/rasa_env_3.4/lib/python3.8/site-packages/urllib3/response.py:877\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    874\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m amt:\n\u001b[1;32m    875\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer\u001b[39m.\u001b[39mget(amt)\n\u001b[0;32m--> 877\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raw_read(amt)\n\u001b[1;32m    879\u001b[0m flush_decoder \u001b[39m=\u001b[39m amt \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m (amt \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m data)\n\u001b[1;32m    881\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m data \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m/workspace/nlplab/quannd/rasa_env_3.4/lib/python3.8/site-packages/urllib3/response.py:812\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    809\u001b[0m fp_closed \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp, \u001b[39m\"\u001b[39m\u001b[39mclosed\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    811\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 812\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp_read(amt) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fp_closed \u001b[39melse\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    813\u001b[0m     \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m amt \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m data:\n\u001b[1;32m    814\u001b[0m         \u001b[39m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[39m# Close the connection when no data is returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[39m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[39m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[1;32m    822\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m/workspace/nlplab/quannd/rasa_env_3.4/lib/python3.8/site-packages/urllib3/response.py:789\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    788\u001b[0m     chunk_amt \u001b[39m=\u001b[39m max_chunk_amt\n\u001b[0;32m--> 789\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp\u001b[39m.\u001b[39;49mread(chunk_amt)\n\u001b[1;32m    790\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m data:\n\u001b[1;32m    791\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/http/client.py:458\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    456\u001b[0m     \u001b[39m# Amount is given, implement using readinto\u001b[39;00m\n\u001b[1;32m    457\u001b[0m     b \u001b[39m=\u001b[39m \u001b[39mbytearray\u001b[39m(amt)\n\u001b[0;32m--> 458\u001b[0m     n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[1;32m    459\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mmemoryview\u001b[39m(b)[:n]\u001b[39m.\u001b[39mtobytes()\n\u001b[1;32m    460\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    461\u001b[0m     \u001b[39m# Amount is not given (unbounded read) so we must check self.length\u001b[39;00m\n\u001b[1;32m    462\u001b[0m     \u001b[39m# and self.chunked\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/http/client.py:502\u001b[0m, in \u001b[0;36mHTTPResponse.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    497\u001b[0m         b \u001b[39m=\u001b[39m \u001b[39mmemoryview\u001b[39m(b)[\u001b[39m0\u001b[39m:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength]\n\u001b[1;32m    499\u001b[0m \u001b[39m# we do not use _safe_read() here because this may be a .will_close\u001b[39;00m\n\u001b[1;32m    500\u001b[0m \u001b[39m# connection, and the user is reading more bytes than will be provided\u001b[39;00m\n\u001b[1;32m    501\u001b[0m \u001b[39m# (for example, reading in 1k chunks)\u001b[39;00m\n\u001b[0;32m--> 502\u001b[0m n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[1;32m    503\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m n \u001b[39mand\u001b[39;00m b:\n\u001b[1;32m    504\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    505\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    506\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m/usr/lib/python3.8/socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    668\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 669\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    670\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    671\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/ssl.py:1241\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1237\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1238\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1239\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1240\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1241\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1242\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1243\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/usr/lib/python3.8/ssl.py:1099\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1098\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1099\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1100\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1101\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "model_path = hf_hub_download(repo_id=\"facebook/fasttext-vi-vectors\", filename=\"model.bin\")\n",
    "model = fasttext.load_model(model_path)\n",
    "model.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vietokenizer\n",
    "import py_vncorenlp\n",
    "from underthesea import text_normalize\n",
    "segmenter = py_vncorenlp.VnCoreNLP(save_dir=\"/workspace/nlplab/quannd/scada-full-stack/chatbot-scada/vncorenlp/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text_normalize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNhấn  Delete  và làm sạch mọi thứ như bạn làm sạch tâm hồn .\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m normalized_text \u001b[38;5;241m=\u001b[39m \u001b[43mtext_normalize\u001b[49m(text)\n\u001b[1;32m      3\u001b[0m words \u001b[38;5;241m=\u001b[39m segmenter(normalized_text)\n\u001b[1;32m      4\u001b[0m words \u001b[38;5;241m=\u001b[39m [word\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text_normalize' is not defined"
     ]
    }
   ],
   "source": [
    "text = \"Nhấn  Delete  và làm sạch mọi thứ như bạn làm sạch tâm hồn .\"\n",
    "normalized_text = text_normalize(text)\n",
    "words = segmenter(normalized_text)\n",
    "words = [word.replace(\"_\", \" \") for word in words.split(\" \")]\n",
    "words = [text_normalize(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tăng chiều',\n",
       " 'dài',\n",
       " 'của',\n",
       " 'hình vuông',\n",
       " 'màu',\n",
       " 'đỏ',\n",
       " 'có',\n",
       " 'kích thước',\n",
       " '250x250 pixel',\n",
       " ',',\n",
       " 'độ',\n",
       " 'dày',\n",
       " 'đường',\n",
       " 'viền',\n",
       " '1',\n",
       " 'pixel',\n",
       " ',',\n",
       " 'nằm',\n",
       " 'ở',\n",
       " 'tọa độ',\n",
       " '(',\n",
       " '10',\n",
       " ',',\n",
       " '10',\n",
       " ')',\n",
       " 'của',\n",
       " 'bản vẽ',\n",
       " 'lên',\n",
       " '15',\n",
       " 'cm',\n",
       " '.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Nhấn  Delete  và làm sạch mọi thứ như bạn làm sạch tâm hồn .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m running_offset \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[43mwords\u001b[49m:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      4\u001b[0m         word_offset \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mindex(word, running_offset)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'words' is not defined"
     ]
    }
   ],
   "source": [
    "running_offset = 0\n",
    "for word in words:\n",
    "    try:\n",
    "        word_offset = text.index(word, running_offset)\n",
    "        word_len = len(word)\n",
    "        running_offset = word_offset + word_len\n",
    "        print(word,\": \", word_offset, \"->\", running_offset)\n",
    "    except:\n",
    "        print(f\"error at word: {word} and substring: {text[running_offset:]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try to test tokenizer in rasa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "            running_offset = 0\n",
    "            tokens = []\n",
    "            for word in words:\n",
    "                word_offset = text.index(word, running_offset)\n",
    "                word_len = len(word)\n",
    "                running_offset = word_offset + word_len\n",
    "                tokens.append(Token(word, word_offset))\n",
    "\n",
    "            return tokens\n",
    "        except:\n",
    "            from underthesea import text_normalize\n",
    "            print(text)\n",
    "            running_offset = 0\n",
    "            tokens = []\n",
    "            new_text = text_normalize(text)\n",
    "            new_words = new_text.split(\" \")\n",
    "\n",
    "            for word in new_words:\n",
    "                word_offset = new_text.index(word, running_offset)\n",
    "                word_len = len(word)\n",
    "                running_offset = word_offset + word_len\n",
    "                tokens.append(Token(word, word_offset))\n",
    "\n",
    "            return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "x = open(\"/workspace/nlplab/quannd/x.yml\", \"r\")\n",
    "y = open(\"/workspace/nlplab/quannd/y.yml\", \"r\")\n",
    "\n",
    "x_yaml = yaml.load(x, Loader=yaml.FullLoader)\n",
    "y_yaml = yaml.load(y, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'intent': 'quan', 'examples': '- fasf\\n- afa'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'intent': 'quan', 'examples': '- re\\n- wdas'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'intent': 'quan', 'examples': '- fasf\\n- afa'},\n",
       " {'intent': 'quan', 'examples': '- re\\n- wdas'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_yaml + y_yaml"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rasa_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
